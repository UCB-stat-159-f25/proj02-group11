{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db21929f-e0b4-4898-a9eb-c91098a2d978",
   "metadata": {},
   "source": [
    "## Part 3: Advanced Text Processing - LDA and BERTopic Topic Modeling (20 pts)\n",
    "\n",
    "**Resources:**\n",
    "- LDA:\n",
    "    - https://medium.com/sayahfares19/text-analysis-topic-modelling-with-spacy-gensim-4cd92ef06e06 \n",
    "    - https://www.kaggle.com/code/faressayah/text-analysis-topic-modeling-with-spacy-gensim#%F0%9F%93%9A-Topic-Modeling (code for previous post)\n",
    "    - https://towardsdatascience.com/topic-modelling-in-python-with-spacy-and-gensim-dc8f7748bdbf/ \n",
    "- BERTopic:\n",
    "    - https://maartengr.github.io/BERTopic/getting_started/visualization/visualize_documents.html#visualize-documents-with-plotly \n",
    "    - https://maartengr.github.io/BERTopic/getting_started/visualization/visualize_topics.html \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71a47df2-e6bf-42a0-8fae-3af9361d9926",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "from bertopic import BERTopic\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "import spacy\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee84460-130b-47df-b0e4-6e907bb807df",
   "metadata": {},
   "source": [
    "### LDA\n",
    "\n",
    "- Train an LDA model with 18 topics\n",
    "- Output the top 10 words for each topic. \n",
    "- Output the topic distribution for the first speech\n",
    "- Make a visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7fb57e-5fa1-456d-a838-4fe6894b69e0",
   "metadata": {},
   "source": [
    "You may use the next two cells to process the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8254203-e7be-4f3a-8398-860c2e99b492",
   "metadata": {},
   "outputs": [],
   "source": [
    "sou = pd.read_csv('data/SOTU.csv')\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3bab11b9-9769-4feb-a92a-abef1170f3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text): \n",
    "    doc = nlp(text) \n",
    "    return [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct and not token.is_space and len(token.lemma_) > 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c8a8957-6f1d-4cae-9b60-3f27a9c744c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all texts - note this takes ~ 5 minutes to run\n",
    "processed_docs = sou['Text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040f6b86-e239-43bd-a56a-606d0afce42f",
   "metadata": {},
   "source": [
    "To train an LDA model, use the LdaModel function that we imported a couple of cells back. The last resource linked under the LDA section is especially useful for walking through the steps we have below. *Note: one of the arguments to the LdaModel function is `random_state` which specifies the random seed for reproducibility. Please set yours to 42. Further, the last resource provided uses `LdaMulticore` which is essentially a parallelizable version of our function `LdaModel`. Use `LdaModel` instead, but the usage will be similar, except you can ignore the `iterations` and `workers` arguments..*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e65413f6-ab01-4214-b921-88e2ded269b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dictionary from processed_docs, which is a list of tokens extracted from our speeches\n",
    "dictionary = Dictionary(processed_docs)\n",
    "# filtering words with too low and too high of a frequency\n",
    "dictionary.filter_extremes(no_below = 2, no_above = 0.5, keep_n = 1000)\n",
    "# Creating our bag of words\n",
    "corpus = [dictionary.doc2bow(doc) for doc in processed_docs] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8a20ed12-8f94-4a65-b697-7a7597218af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train LDA model with 18 topics\n",
    "lda_model = LdaModel(corpus = corpus, id2word = dictionary, num_topics = 18, random_state = 42, passes = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "672dfada-454b-41aa-9dea-77e0916d9b68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.010*\"island\" + 0.005*\"mexico\" + 0.004*\"convention\" + 0.004*\"canal\" + 0.004*\"spain\" + 0.004*\"port\" + 0.004*\"chinese\" + 0.004*\"article\" + 0.003*\"admit\" + 0.003*\"corporation\"'),\n",
       " (1,\n",
       "  '0.011*\"method\" + 0.010*\"board\" + 0.009*\"agricultural\" + 0.009*\"cent\" + 0.008*\"farmer\" + 0.008*\"project\" + 0.008*\"tariff\" + 0.007*\"committee\" + 0.007*\"loan\" + 0.007*\"conference\"'),\n",
       " (2,\n",
       "  '0.028*\"americans\" + 0.022*\"tonight\" + 0.012*\"today\" + 0.011*\"thank\" + 0.011*\"budget\" + 0.010*\"percent\" + 0.010*\"program\" + 0.009*\"challenge\" + 0.009*\"worker\" + 0.009*\"hard\"'),\n",
       " (3,\n",
       "  '0.010*\"cent\" + 0.009*\"june\" + 0.008*\"indian\" + 0.007*\"pension\" + 0.006*\"method\" + 0.006*\"indians\" + 0.005*\"mail\" + 0.005*\"postal\" + 0.005*\"amount\" + 0.005*\"bond\"'),\n",
       " (4,\n",
       "  '0.007*\"gold\" + 0.005*\"note\" + 0.004*\"wrong\" + 0.004*\"currency\" + 0.004*\"bond\" + 0.003*\"island\" + 0.003*\"reserve\" + 0.003*\"convention\" + 0.003*\"spain\" + 0.003*\"americans\"'),\n",
       " (5,\n",
       "  '0.007*\"americans\" + 0.006*\"program\" + 0.006*\"tonight\" + 0.005*\"today\" + 0.003*\"billion\" + 0.003*\"thank\" + 0.003*\"budget\" + 0.003*\"mexico\" + 0.003*\"method\" + 0.003*\"goal\"'),\n",
       " (6,\n",
       "  '0.008*\"minister\" + 0.007*\"article\" + 0.006*\"british\" + 0.006*\"intercourse\" + 0.006*\"convention\" + 0.005*\"france\" + 0.005*\"spain\" + 0.005*\"tribe\" + 0.005*\"mexico\" + 0.005*\"port\"'),\n",
       " (7,\n",
       "  '0.019*\"currency\" + 0.010*\"paper\" + 0.009*\"note\" + 0.008*\"surplus\" + 0.008*\"bond\" + 0.007*\"gold\" + 0.007*\"cent\" + 0.007*\"taxation\" + 0.006*\"deposit\" + 0.005*\"evil\"'),\n",
       " (8,\n",
       "  '0.018*\"satisfaction\" + 0.017*\"facilitate\" + 0.017*\"legislature\" + 0.017*\"derive\" + 0.016*\"blessing\" + 0.015*\"intercourse\" + 0.015*\"expedient\" + 0.015*\"paper\" + 0.014*\"uniform\" + 0.014*\"encouragement\"'),\n",
       " (9,\n",
       "  '0.070*\"mexico\" + 0.031*\"texas\" + 0.024*\"mexican\" + 0.008*\"convention\" + 0.008*\"loan\" + 0.007*\"minister\" + 0.007*\"statute\" + 0.006*\"company\" + 0.006*\"article\" + 0.006*\"tariff\"'),\n",
       " (10,\n",
       "  '0.020*\"program\" + 0.019*\"today\" + 0.012*\"democratic\" + 0.012*\"democracy\" + 0.010*\"expression\" + 0.010*\"revolution\" + 0.009*\"americans\" + 0.009*\"ahead\" + 0.009*\"threat\" + 0.008*\"hemisphere\"'),\n",
       " (11,\n",
       "  '0.016*\"island\" + 0.016*\"gold\" + 0.012*\"cuba\" + 0.011*\"spain\" + 0.009*\"june\" + 0.009*\"note\" + 0.008*\"convention\" + 0.007*\"bond\" + 0.007*\"july\" + 0.007*\"spanish\"'),\n",
       " (12,\n",
       "  '0.037*\"democracy\" + 0.013*\"budget\" + 0.013*\"method\" + 0.013*\"billion\" + 0.012*\"loan\" + 0.012*\"prevail\" + 0.012*\"privilege\" + 0.011*\"program\" + 0.010*\"railroad\" + 0.010*\"recovery\"'),\n",
       " (13,\n",
       "  '0.004*\"mexico\" + 0.004*\"convention\" + 0.003*\"june\" + 0.003*\"program\" + 0.003*\"cent\" + 0.003*\"british\" + 0.003*\"note\" + 0.003*\"loan\" + 0.003*\"minister\" + 0.002*\"island\"'),\n",
       " (14,\n",
       "  '0.013*\"task\" + 0.013*\"seven\" + 0.012*\"thousand\" + 0.011*\"play\" + 0.010*\"preparation\" + 0.009*\"generation\" + 0.009*\"hemisphere\" + 0.009*\"promptly\" + 0.008*\"undertake\" + 0.008*\"cent\"'),\n",
       " (15,\n",
       "  '0.011*\"corporation\" + 0.008*\"industrial\" + 0.007*\"railroad\" + 0.007*\"forest\" + 0.007*\"canal\" + 0.007*\"tariff\" + 0.007*\"method\" + 0.007*\"island\" + 0.005*\"body\" + 0.005*\"conference\"'),\n",
       " (16,\n",
       "  '0.038*\"enemy\" + 0.020*\"british\" + 0.013*\"command\" + 0.012*\"fight\" + 0.009*\"militia\" + 0.008*\"battle\" + 0.007*\"germany\" + 0.007*\"task\" + 0.007*\"wrong\" + 0.006*\"soldier\"'),\n",
       " (17,\n",
       "  '0.040*\"program\" + 0.015*\"billion\" + 0.012*\"budget\" + 0.011*\"area\" + 0.010*\"major\" + 0.009*\"level\" + 0.009*\"inflation\" + 0.008*\"today\" + 0.008*\"farm\" + 0.007*\"achieve\"')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.print_topics(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a108849f-8f69-4737-ac89-ed9399f2418f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, np.float32(0.9988718))]\n"
     ]
    }
   ],
   "source": [
    "# print the topic distribution for the first speech\n",
    "first_speech = corpus[0]\n",
    "topic_dist = lda_model.get_document_topics(first_speech)\n",
    "print(topic_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "63c9825b-3dee-4a90-969a-79331035d51b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a visualization using pyLDAvis\n",
    "pyLDAvis.enable_notebook()\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d5f4e3-092a-4927-8516-4a136f2e67c1",
   "metadata": {},
   "source": [
    "### BERTopic\n",
    "\n",
    "- Train a BERTopic model with a `min_topic_size` of 3 *Hint: use `BERTopic` to instantiate the model and specify `min_topic_size` in here. Actually fit the model using `fit_transform`, which `docs` passed into this.*\n",
    "- Output the top 10 words for each topic. \n",
    "- Output the topic distribution for the first speech\n",
    "- Make a visualization of the topics (see topic_model.visualize_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c18d54c4-3959-4226-b0ef-2dfab390133b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sou' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m docs = \u001b[43msou\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mText\u001b[39m\u001b[33m'\u001b[39m].to_list()\n",
      "\u001b[31mNameError\u001b[39m: name 'sou' is not defined"
     ]
    }
   ],
   "source": [
    "docs = sou['Text'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c59237c-3ce1-40b0-a269-ea81a8f6dca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model - this takes about 30 seconds\n",
    "\n",
    "# remove stop words from the topics (Hint: use CountVectorizer and then .update_topics on topic_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4572431a-49b2-40d8-9a89-610f0ecf02fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output the top 10 words for each topic - hint see get_topic_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d36e7424-f002-47ea-bb3f-069fdda99f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output the topic distribution for the first speech\n",
    "# hint: check out approximate_distribution() and visualize_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "043ffd99-96dc-4a84-b311-bb20e27dbaac",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'topic_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# run this cell to visualize the topics\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mtopic_model\u001b[49m.visualize_topics()\n",
      "\u001b[31mNameError\u001b[39m: name 'topic_model' is not defined"
     ]
    }
   ],
   "source": [
    "# run this cell to visualize the topics\n",
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68d2385-f0a8-49e4-8a14-2f1d20fdb56f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPython - sotu",
   "language": "python",
   "name": "sotu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
